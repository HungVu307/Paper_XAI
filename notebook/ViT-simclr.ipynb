{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8566029,"sourceType":"datasetVersion","datasetId":5121102},{"sourceId":8634739,"sourceType":"datasetVersion","datasetId":5170605},{"sourceId":62544,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52230}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport cv2\nimport os\nfrom torchvision import transforms\nimport pandas as pd\nfrom torch import nn\nfrom torchvision.transforms import transforms\n\nfrom torch.utils.data._utils.collate import default_collate\n\n\ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nclass GaussianBlur(object):\n    \"\"\"blur a single image on CPU\"\"\"\n    def __init__(self, kernel_size):\n        radias = kernel_size // 2\n        kernel_size = radias * 2 + 1\n        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.k = kernel_size\n        self.r = radias\n\n        self.blur = nn.Sequential(\n            nn.ReflectionPad2d(radias),\n            self.blur_h,\n            self.blur_v\n        )\n\n        self.pil_to_tensor = transforms.ToTensor()\n        self.tensor_to_pil = transforms.ToPILImage()\n\n    def __call__(self, img):\n        img = self.pil_to_tensor(img).unsqueeze(0)\n\n        sigma = np.random.uniform(0.1, 2.0)\n        x = np.arange(-self.r, self.r + 1)\n        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n        x = x / x.sum()\n        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n\n        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n\n        with torch.no_grad():\n            img = self.blur(img)\n            img = img.squeeze()\n\n        img = self.tensor_to_pil(img)\n\n        return img\n    \nclass ContrastiveLearningViewGenerator(object):\n    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n\n    def __init__(self, base_transform, n_views=2):\n        self.base_transform = base_transform\n        self.n_views = n_views\n\n    def __call__(self, x):\n        return [self.base_transform(x) for i in range(self.n_views)]\n\n\nclass MammoDataset(Dataset):\n    @staticmethod\n    def get_simclr_pipeline_transform(size, s=1):\n        \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n        data_transforms = transforms.Compose([transforms.ToPILImage(),\n                                              transforms.RandomResizedCrop(size=size),\n                                              transforms.RandomHorizontalFlip(),\n                                              transforms.RandomApply([color_jitter], p=0.8),\n                                              transforms.RandomGrayscale(p=0.2),\n                                              GaussianBlur(kernel_size=int(0.1 * size)),\n                                              transforms.ToTensor()])\n        return data_transforms\n    \n    def __init__(self,\n                 data_path,\n                 metadata,\n                 phase,\n                 trasnform=None,\n                 certain=True,\n                 image_size=224,\n                 mask_patch_size=32,\n                 model_patch_size=16,\n                 mask_ratio=0.6,\n                 seed=None):\n        self.phase = phase\n        self.data_path = data_path\n\n        if (seed):\n            seed_everything(seed)\n\n        self.transform = ContrastiveLearningViewGenerator(self.get_simclr_pipeline_transform(224), 2)\n        data = pd.read_csv(metadata)\n        self.data = data.loc[data[\"split\"] == phase].reset_index()\n\n    def get_path(self, data, index):\n        image_name = data['image_id'].iloc[index]\n        study_id = data['study_id'].iloc[index]\n        image_path = os.path.join(self.data_path, study_id + '/' + image_name + '.png')\n        return image_path\n\n    def get_score(self, data, index):\n        birads = data['breast_birads'].iloc[index]\n        score = eval(birads[-1])\n        return score\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        image_path = self.get_path(self.data, index)\n        image = cv2.imread(image_path)\n        image = self.transform(image)\n        score = self.get_score(self.data, index)\n        return image, score","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.299419Z","iopub.execute_input":"2024-06-09T15:31:31.299833Z","iopub.status.idle":"2024-06-09T15:31:31.323731Z","shell.execute_reply.started":"2024-06-09T15:31:31.299800Z","shell.execute_reply":"2024-06-09T15:31:31.322224Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model","metadata":{}},{"cell_type":"markdown","source":"## 2.1 - ViT Encoder","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on BEIT code bases (https://github.com/microsoft/unilm/tree/master/beit)\n# Written by Yutong Lin, Zhenda Xie\n# --------------------------------------------------------\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # comment out this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            # cls to token & token to cls & cls to cls\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n                 use_mean_pooling=True, init_scale=0.001):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n\n        if self.pos_embed is not None:\n            self._trunc_normal_(self.pos_embed, std=.02)\n        self._trunc_normal_(self.cls_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def _trunc_normal_(self, tensor, mean=0., std=1.):\n        trunc_normal_(tensor, mean=mean, std=std)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            self._trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            self._trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        x = self.norm(x)\n        if self.fc_norm is not None:\n            t = x[:, 1:, :]\n            return self.fc_norm(t.mean(1))\n        else:\n            return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef build_vit(config):\n    model = VisionTransformer(\n        img_size=config.DATA.IMG_SIZE,\n        patch_size=config.MODEL.VIT.PATCH_SIZE,\n        in_chans=config.MODEL.VIT.IN_CHANS,\n        embed_dim=config.MODEL.VIT.EMBED_DIM,\n        depth=config.MODEL.VIT.DEPTH,\n        num_heads=config.MODEL.VIT.NUM_HEADS,\n        mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n        qkv_bias=config.MODEL.VIT.QKV_BIAS,\n        drop_rate=config.MODEL.DROP_RATE,\n        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=config.MODEL.VIT.INIT_VALUES,\n        use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n        use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n        use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n        use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.528481Z","iopub.execute_input":"2024-06-09T15:31:31.528919Z","iopub.status.idle":"2024-06-09T15:31:31.598092Z","shell.execute_reply.started":"2024-06-09T15:31:31.528883Z","shell.execute_reply":"2024-06-09T15:31:31.596726Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 - SimCLR model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SimCLR(nn.Module):\n\n    def __init__(self, base_model, config, out_dim):\n        super(SimCLR, self).__init__()\n        self.resnet_dict = {\"vit\": build_vit(config),\n                            \"swin\": None}\n\n        self.backbone = self._get_basemodel(base_model)\n        dim_mlp = self.backbone.embed_dim\n\n        # add mlp projection head\n        self.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(dim_mlp, 512),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(512, out_dim)\n                                )\n\n    def _get_basemodel(self, model_name):\n        try:\n            model = self.resnet_dict[model_name]\n        except Exception:\n            print(\"Invalid backbone architecture. Check the config file and pass one of: Vit or Swin Transformer\")\n        else:\n            return model\n\n    def forward(self, x):\n        out = self.backbone(x)\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.600357Z","iopub.execute_input":"2024-06-09T15:31:31.600775Z","iopub.status.idle":"2024-06-09T15:31:31.613063Z","shell.execute_reply.started":"2024-06-09T15:31:31.600741Z","shell.execute_reply":"2024-06-09T15:31:31.611697Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# 3. Loss function (Cross Entropy)","metadata":{}},{"cell_type":"markdown","source":"# 4. Training Pipeline","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport os\n\ndef info_nce_loss(features, batch_size, n_views, temperature, device):\n    labels = torch.cat([torch.arange(batch_size) for i in range(n_views)], dim=0)\n    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n    labels = labels.to(device)\n\n    features = F.normalize(features, dim=1)\n\n    similarity_matrix = torch.matmul(features, features.T)\n    # assert similarity_matrix.shape == (\n    #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)\n    # assert similarity_matrix.shape == labels.shape\n\n    # discard the main diagonal from both: labels and similarities matrix\n    mask = torch.eye(labels.shape[0], dtype=torch.bool).to(device)\n    labels = labels[~mask].view(labels.shape[0], -1)\n    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n    # assert similarity_matrix.shape == labels.shape\n\n    # select and combine multiple positives\n    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n\n    # select only the negatives the negatives\n    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n\n    logits = torch.cat([positives, negatives], dim=1)\n    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n\n    logits = logits / temperature\n    return logits, labels\n    \ndef train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32,\n                learning_rate=0.001, temperature = 0.07):\n    \"\"\"\n    Train the model using the provided datasets.\n\n    Args:\n    - model: The model to be trained\n    - train_dataset: Dataset for training\n    - val_dataset: Dataset for validation\n    - checkpoint_folder: Folder to store checkpoints\n    - num_epochs: Number of epochs for training\n    - batch_size: Batch size for training\n    - learning_rate: Learning rate for optimization\n\n    Returns:\n    - model: Trained model\n    - train_losses: List of training losses\n    - val_losses: List of validation losses\n    \"\"\"\n    # Create the checkpoint folder if it doesn't exist\n    if not os.path.exists(checkpoint_folder):\n        os.makedirs(checkpoint_folder)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    # Define data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Define loss function and optimizer\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n    # Lists to store training and validation losses\n    train_losses = []\n    val_losses = []\n\n    # Variables to keep track of the best model and its performance\n    best_val_loss = float('inf')\n    best_model_state = None\n\n    model = model.to(device)\n    print(\"Training started...\")\n    for epoch in range(num_epochs):\n        torch.cuda.empty_cache()\n        print(\"*\" * 100)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}]:\")\n        model.train()\n        running_train_loss = 0.0\n        for i, (images, _) in enumerate(train_loader):\n            optimizer.zero_grad()\n            # Forward pass\n            images = torch.cat(images, dim=0)\n            images = images.to(device)\n            features = model(images)\n            logits, labels = info_nce_loss(features, batch_size, 2, temperature, device)\n\n            # Compute loss\n            loss = criterion(logits, labels)\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            running_train_loss += loss.item()\n\n            if i % 200 == 0:\n                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n\n        # Compute average training loss for the epoch\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # Validation loop\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for i, (images, _) in enumerate(val_loader):\n                images = torch.cat(images, dim=0)\n                images = images.to(device)\n                features = model(images)\n                logits, labels = info_nce_loss(features, batch_size, 2, temperature, device)\n\n                # Compute loss\n                loss = criterion(logits, labels)\n                running_val_loss += loss.item()\n\n                if i % 100 == 0:\n                    print(\n                        f\"Epoch [{epoch + 1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n\n        # Compute average validation loss for the epoch\n        epoch_val_loss = running_val_loss / len(val_loader)\n        val_losses.append(epoch_val_loss)\n\n        # Save the model checkpoint for every epoch (last model)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': epoch_val_loss\n        }, os.path.join(checkpoint_folder, f'last.pt'))\n\n        # Save the best model checkpoint based on validation loss\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            best_model_state = model.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': best_model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss\n            }, os.path.join(checkpoint_folder, f'best.pt'))\n\n        # Print progress\n        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n        print(\"*\" * 100)\n        scheduler.step()\n    print(\"Training completed.\")\n\n    return model, train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.616391Z","iopub.execute_input":"2024-06-09T15:31:31.617002Z","iopub.status.idle":"2024-06-09T15:31:31.651668Z","shell.execute_reply.started":"2024-06-09T15:31:31.616951Z","shell.execute_reply":"2024-06-09T15:31:31.649214Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# 5. Experiments","metadata":{}},{"cell_type":"code","source":"import datetime\nnow = datetime.datetime.now()\n\nconfig = {\n    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n    \"model_encoder\": \"vit\",\n    \"embedding_dim\": 256, \n    \"learning_rate\":0.1,\n    \"num_epoch\": 50,\n    \"batch_size\": 8,\n    \"temperature\": 0.07,\n    \"model_config\": \"/kaggle/input/vit-config/simmim_pretrain__vit_base__img224__800ep.yaml\",\n    \"checkpoint\": \"/kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\",\n    \"checkpoint_folder\": f\"/kaggle/working/weights_setting2/resnet50BasedModel_{now}\"\n}\n\nclass AttrDict(dict):\n    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for key, value in self.items():\n            if isinstance(value, dict):\n                value = AttrDict(value)\n            self[key] = value\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError(f\"'AttrDict' object has no attribute '{item}'\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.653321Z","iopub.execute_input":"2024-06-09T15:31:31.653820Z","iopub.status.idle":"2024-06-09T15:31:31.671682Z","shell.execute_reply.started":"2024-06-09T15:31:31.653771Z","shell.execute_reply":"2024-06-09T15:31:31.669748Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import yaml\n\ntrain_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"training\",\n                                seed=0)\nvalid_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"valid\",\n                                seed=0)\nwith open(config[\"model_config\"], 'r') as file:\n    data = yaml.safe_load(file)\nmodel_config = AttrDict(data)\nmodel = SimCLR(\"vit\", model_config, out_dim=config[\"embedding_dim\"])\n\nif config[\"checkpoint\"]:\n    checkpoint = torch.load(config[\"checkpoint\"])\n    print(\"Checkpoint: {}\".format(config[\"checkpoint\"]))\n    model.load_state_dict(checkpoint, strict = False)\n\ntrain_model(model=model, train_dataset=train_dataset,\n            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n            checkpoint_folder=config[\"checkpoint_folder\"],\n            temperature = config[\"temperature\"]\n            )","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:31:31.674460Z","iopub.execute_input":"2024-06-09T15:31:31.674915Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Checkpoint: /kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\nDevice: cpu\nTraining started...\n****************************************************************************************************\nEpoch [1/50]:\n\t Batch [0/1600], Train Loss: 2.7035\n","output_type":"stream"}]}]}