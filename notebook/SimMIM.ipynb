{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4e29bc",
   "metadata": {
    "papermill": {
     "duration": 0.005022,
     "end_time": "2024-06-09T08:24:58.353716",
     "exception": false,
     "start_time": "2024-06-09T08:24:58.348694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803d0181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:24:58.364668Z",
     "iopub.status.busy": "2024-06-09T08:24:58.364294Z",
     "iopub.status.idle": "2024-06-09T08:25:03.909980Z",
     "shell.execute_reply": "2024-06-09T08:25:03.909232Z"
    },
    "papermill": {
     "duration": 5.553635,
     "end_time": "2024-06-09T08:25:03.912224",
     "exception": false,
     "start_time": "2024-06-09T08:24:58.358589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "mean = [0.2652, 0.2652, 0.2652]\n",
    "std = [0.1994, 0.1994, 0.1994]\n",
    "data_transforms = {\n",
    "    'training': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3),\n",
    "        transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3), ]), p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "\n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "\n",
    "        self.token_count = self.rand_size ** 2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "\n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "\n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "class MammoDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 metadata,\n",
    "                 phase,\n",
    "                 trasnform=None,\n",
    "                 certain=True,\n",
    "                 image_size=224,\n",
    "                 mask_patch_size=32,\n",
    "                 model_patch_size=16,\n",
    "                 mask_ratio=0.6,\n",
    "                 seed=None):\n",
    "        self.phase = phase\n",
    "        self.certain = certain\n",
    "        self.data_path = data_path\n",
    "\n",
    "        if (seed):\n",
    "            seed_everything(seed)\n",
    "\n",
    "        self.transform = data_transforms[self.phase] if (trasnform is None) else trasnform\n",
    "        data = pd.read_csv(metadata)\n",
    "        self.data = data.loc[data[\"split\"] == phase].reset_index()\n",
    "        self.mask_generator = MaskGenerator(\n",
    "            input_size=image_size,\n",
    "            mask_patch_size=mask_patch_size,\n",
    "            model_patch_size=model_patch_size,\n",
    "            mask_ratio=mask_ratio,\n",
    "        )\n",
    "\n",
    "    def get_path(self, data, index):\n",
    "        image_name = data['image_id'].iloc[index]\n",
    "        study_id = data['study_id'].iloc[index]\n",
    "        image_path = os.path.join(self.data_path, study_id + '/' + image_name + '.png')\n",
    "        return image_path\n",
    "\n",
    "    def get_score(self, data, index):\n",
    "        birads = data['breast_birads'].iloc[index]\n",
    "        score = eval(birads[-1])\n",
    "        return score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.get_path(self.data, index)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = self.transform(image)\n",
    "        mask = self.mask_generator()\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "        score = self.get_score(self.data, index)\n",
    "        return image, mask, score\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not isinstance(batch[0][0], tuple):\n",
    "        return default_collate(batch)\n",
    "    else:\n",
    "        batch_num = len(batch)\n",
    "        ret = []\n",
    "        for item_idx in range(len(batch[0][0])):\n",
    "            if batch[0][0][item_idx] is None:\n",
    "                ret.append(None)\n",
    "            else:\n",
    "                ret.append(default_collate([batch[i][0][item_idx] for i in range(batch_num)]))\n",
    "        ret.append(default_collate([batch[i][1] for i in range(batch_num)]))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489414f",
   "metadata": {
    "papermill": {
     "duration": 0.004616,
     "end_time": "2024-06-09T08:25:03.922266",
     "exception": false,
     "start_time": "2024-06-09T08:25:03.917650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcca363",
   "metadata": {
    "papermill": {
     "duration": 0.004295,
     "end_time": "2024-06-09T08:25:03.930993",
     "exception": false,
     "start_time": "2024-06-09T08:25:03.926698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 - ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2e70c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:25:03.941604Z",
     "iopub.status.busy": "2024-06-09T08:25:03.941186Z",
     "iopub.status.idle": "2024-06-09T08:25:05.139885Z",
     "shell.execute_reply": "2024-06-09T08:25:05.139099Z"
    },
    "papermill": {
     "duration": 1.206728,
     "end_time": "2024-06-09T08:25:05.142148",
     "exception": false,
     "start_time": "2024-06-09T08:25:03.935420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# SimMIM\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Based on BEIT code bases (https://github.com/microsoft/unilm/tree/master/beit)\n",
    "# Written by Yutong Lin, Zhenda Xie\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # comment out this for the orignal BERT implement\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0., window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        if window_size:\n",
    "            self.window_size = window_size\n",
    "            # cls to token & token to cls & cls to cls\n",
    "            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(window_size[0])\n",
    "            coords_w = torch.arange(window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "            relative_position_index = \\\n",
    "                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n",
    "            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "            relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "            relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        else:\n",
    "            self.window_size = None\n",
    "            self.relative_position_bias_table = None\n",
    "            self.relative_position_index = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if self.relative_position_bias_table is not None:\n",
    "            relative_position_bias = \\\n",
    "                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                    self.window_size[0] * self.window_size[1] + 1,\n",
    "                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if rel_pos_bias is not None:\n",
    "            attn = attn + rel_pos_bias\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        # cls to token & token 2 cls & cls to cls\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(window_size[0])\n",
    "        coords_w = torch.arange(window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "        relative_position_index = \\\n",
    "            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n",
    "        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "        relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "        relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def forward(self):\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size[0] * self.window_size[1] + 1,\n",
    "                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n",
    "                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n",
    "                 use_mean_pooling=True, init_scale=0.001):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        if use_abs_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if use_shared_rel_pos_bias:\n",
    "            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n",
    "        else:\n",
    "            self.rel_pos_bias = None\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.use_rel_pos_bias = use_rel_pos_bias\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n",
    "            for i in range(depth)])\n",
    "        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n",
    "\n",
    "        if self.pos_embed is not None:\n",
    "            self._trunc_normal_(self.pos_embed, std=.02)\n",
    "        self._trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def _trunc_normal_(self, tensor, mean=0., std=1.):\n",
    "        trunc_normal_(tensor, mean=mean, std=std)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self._trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            self._trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rel_pos_bias=rel_pos_bias)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        if self.fc_norm is not None:\n",
    "            t = x[:, 1:, :]\n",
    "            return self.fc_norm(t.mean(1))\n",
    "        else:\n",
    "            return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_vit(config):\n",
    "    model = VisionTransformer(\n",
    "        img_size=config.DATA.IMG_SIZE,\n",
    "        patch_size=config.MODEL.VIT.PATCH_SIZE,\n",
    "        in_chans=config.MODEL.VIT.IN_CHANS,\n",
    "        embed_dim=config.MODEL.VIT.EMBED_DIM,\n",
    "        depth=config.MODEL.VIT.DEPTH,\n",
    "        num_heads=config.MODEL.VIT.NUM_HEADS,\n",
    "        mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n",
    "        qkv_bias=config.MODEL.VIT.QKV_BIAS,\n",
    "        drop_rate=config.MODEL.DROP_RATE,\n",
    "        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        init_values=config.MODEL.VIT.INIT_VALUES,\n",
    "        use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n",
    "        use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n",
    "        use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n",
    "        use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141d9aa",
   "metadata": {
    "papermill": {
     "duration": 0.004282,
     "end_time": "2024-06-09T08:25:05.151170",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.146888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 - SimMIM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b3c5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:25:05.161584Z",
     "iopub.status.busy": "2024-06-09T08:25:05.161287Z",
     "iopub.status.idle": "2024-06-09T08:25:05.188513Z",
     "shell.execute_reply": "2024-06-09T08:25:05.187805Z"
    },
    "papermill": {
     "duration": 0.034808,
     "end_time": "2024-06-09T08:25:05.190366",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.155558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# SimMIM\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Zhenda Xie\n",
    "# --------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "\n",
    "\n",
    "# class SwinTransformerForSimMIM(SwinTransformer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "\n",
    "#         assert self.num_classes == 0\n",
    "\n",
    "#         self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "#         trunc_normal_(self.mask_token, mean=0., std=.02)\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         x = self.patch_embed(x)\n",
    "\n",
    "#         assert mask is not None\n",
    "#         B, L, _ = x.shape\n",
    "\n",
    "#         mask_tokens = self.mask_token.expand(B, L, -1)\n",
    "#         w = mask.flatten(1).unsqueeze(-1).type_as(mask_tokens)\n",
    "#         x = x * (1. - w) + mask_tokens * w\n",
    "\n",
    "#         if self.ape:\n",
    "#             x = x + self.absolute_pos_embed\n",
    "#         x = self.pos_drop(x)\n",
    "\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         x = self.norm(x)\n",
    "\n",
    "#         x = x.transpose(1, 2)\n",
    "#         B, C, L = x.shape\n",
    "#         H = W = int(L ** 0.5)\n",
    "#         x = x.reshape(B, C, H, W)\n",
    "#         return x\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def no_weight_decay(self):\n",
    "#         return super().no_weight_decay() | {'mask_token'}\n",
    "\n",
    "\n",
    "class VisionTransformerForSimMIM(VisionTransformer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        self._trunc_normal_(self.mask_token, std=.02)\n",
    "\n",
    "    def _trunc_normal_(self, tensor, mean=0., std=1.):\n",
    "        trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        assert mask is not None\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        mask_token = self.mask_token.expand(B, L, -1)\n",
    "        w = mask.flatten(1).unsqueeze(-1).type_as(mask_token)\n",
    "        x = x * (1 - w) + mask_token * w\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rel_pos_bias=rel_pos_bias)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = x[:, 1:]\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)\n",
    "        x = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimMIM(nn.Module):\n",
    "    def __init__(self, encoder, encoder_stride):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder_stride = encoder_stride\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.encoder.num_features,\n",
    "                out_channels=self.encoder_stride ** 2 * 3, kernel_size=1),\n",
    "            nn.PixelShuffle(self.encoder_stride),\n",
    "        )\n",
    "\n",
    "        self.in_chans = self.encoder.in_chans\n",
    "        self.patch_size = self.encoder.patch_size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        z = self.encoder(x, mask)\n",
    "        x_rec = self.decoder(z)\n",
    "        ## Fix size of mask\n",
    "        mask = mask.repeat_interleave(self.patch_size, 2).repeat_interleave(self.patch_size, 3)\n",
    "        loss_recon = F.l1_loss(x, x_rec, reduction='none')\n",
    "        loss = (loss_recon * mask).sum() / (mask.sum() + 1e-5) / self.in_chans\n",
    "        return loss\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        if hasattr(self.encoder, 'no_weight_decay'):\n",
    "            return {'encoder.' + i for i in self.encoder.no_weight_decay()}\n",
    "        return {}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        if hasattr(self.encoder, 'no_weight_decay_keywords'):\n",
    "            return {'encoder.' + i for i in self.encoder.no_weight_decay_keywords()}\n",
    "        return {}\n",
    "\n",
    "\n",
    "def build_simmim(config):\n",
    "    model_type = config.MODEL.TYPE\n",
    "    if model_type == 'swin':\n",
    "        encoder = SwinTransformerForSimMIM(\n",
    "            img_size=config.DATA.IMG_SIZE,\n",
    "            patch_size=config.MODEL.SWIN.PATCH_SIZE,\n",
    "            in_chans=config.MODEL.SWIN.IN_CHANS,\n",
    "            embed_dim=config.MODEL.SWIN.EMBED_DIM,\n",
    "            depths=config.MODEL.SWIN.DEPTHS,\n",
    "            num_heads=config.MODEL.SWIN.NUM_HEADS,\n",
    "            window_size=config.MODEL.SWIN.WINDOW_SIZE,\n",
    "            mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n",
    "            qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n",
    "            qk_scale=config.MODEL.SWIN.QK_SCALE,\n",
    "            drop_rate=config.MODEL.DROP_RATE,\n",
    "            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "            ape=config.MODEL.SWIN.APE,\n",
    "            patch_norm=config.MODEL.SWIN.PATCH_NORM,\n",
    "            use_checkpoint=config.TRAIN.USE_CHECKPOINT)\n",
    "        encoder_stride = 32\n",
    "    elif model_type == 'vit':\n",
    "        encoder = VisionTransformerForSimMIM(\n",
    "            img_size=config.DATA.IMG_SIZE,\n",
    "            patch_size=config.MODEL.VIT.PATCH_SIZE,\n",
    "            in_chans=config.MODEL.VIT.IN_CHANS,\n",
    "            embed_dim=config.MODEL.VIT.EMBED_DIM,\n",
    "            depth=config.MODEL.VIT.DEPTH,\n",
    "            num_heads=config.MODEL.VIT.NUM_HEADS,\n",
    "            mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n",
    "            qkv_bias=config.MODEL.VIT.QKV_BIAS,\n",
    "            drop_rate=config.MODEL.DROP_RATE,\n",
    "            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            init_values=config.MODEL.VIT.INIT_VALUES,\n",
    "            use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n",
    "            use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n",
    "            use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n",
    "            use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n",
    "        encoder_stride = 16\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown pre-train model: {model_type}\")\n",
    "\n",
    "    model = SimMIM(encoder=encoder, encoder_stride=encoder_stride)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72784b",
   "metadata": {
    "papermill": {
     "duration": 0.004184,
     "end_time": "2024-06-09T08:25:05.198999",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.194815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e2392",
   "metadata": {
    "papermill": {
     "duration": 0.004266,
     "end_time": "2024-06-09T08:25:05.207709",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.203443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aee767a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:25:05.218092Z",
     "iopub.status.busy": "2024-06-09T08:25:05.217833Z",
     "iopub.status.idle": "2024-06-09T08:25:05.235471Z",
     "shell.execute_reply": "2024-06-09T08:25:05.234633Z"
    },
    "papermill": {
     "duration": 0.02525,
     "end_time": "2024-06-09T08:25:05.237367",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.212117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32,\n",
    "                learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the model using the provided datasets.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to be trained\n",
    "    - train_dataset: Dataset for training\n",
    "    - val_dataset: Dataset for validation\n",
    "    - checkpoint_folder: Folder to store checkpoints\n",
    "    - num_epochs: Number of epochs for training\n",
    "    - batch_size: Batch size for training\n",
    "    - learning_rate: Learning rate for optimization\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - train_losses: List of training losses\n",
    "    - val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Create the checkpoint folder if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_folder):\n",
    "        os.makedirs(checkpoint_folder)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    # Define data loaders for training and validation\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Variables to keep track of the best model and its performance\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]:\")\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for i, (img, mask, _) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            img = img.to(device)\n",
    "            mask = mask.to(device)\n",
    "            loss = model(img, mask)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Compute average training loss for the epoch\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (img, mask, _) in enumerate(val_loader):\n",
    "                img = img.to(device)\n",
    "                mask = mask.to(device)\n",
    "                loss = model(img, mask)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Compute average validation loss for the epoch\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # Save the model checkpoint for every epoch (last model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': epoch_val_loss\n",
    "        }, os.path.join(checkpoint_folder, f'last.pt'))\n",
    "\n",
    "        # Save the best model checkpoint based on validation loss\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss\n",
    "            }, os.path.join(checkpoint_folder, f'best.pt'))\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "        print(\"*\" * 100)\n",
    "        scheduler.step()\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8ebbe",
   "metadata": {
    "papermill": {
     "duration": 0.004162,
     "end_time": "2024-06-09T08:25:05.245994",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.241832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7700d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:25:05.255766Z",
     "iopub.status.busy": "2024-06-09T08:25:05.255469Z",
     "iopub.status.idle": "2024-06-09T08:25:05.262785Z",
     "shell.execute_reply": "2024-06-09T08:25:05.262054Z"
    },
    "papermill": {
     "duration": 0.014364,
     "end_time": "2024-06-09T08:25:05.264733",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.250369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "config = {\n",
    "    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n",
    "    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n",
    "    \"model_encoder\": \"vit\",\n",
    "    \"data_length\": 100000,\n",
    "    \"embedding_dim\": 256, \n",
    "    \"learning_rate\":0.1,\n",
    "    \"num_epoch\": 50,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_config\": \"/kaggle/input/vit-config/simmim_pretrain__vit_base__img224__800ep.yaml\",\n",
    "    \"checkpoint\": \"/kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\",\n",
    "    \"checkpoint_folder\": f\"/kaggle/working/weights_setting2/resnet50BasedModel_{now}\"\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for key, value in self.items():\n",
    "            if isinstance(value, dict):\n",
    "                value = AttrDict(value)\n",
    "            self[key] = value\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'AttrDict' object has no attribute '{item}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6923cfdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T08:25:05.274634Z",
     "iopub.status.busy": "2024-06-09T08:25:05.274356Z",
     "iopub.status.idle": "2024-06-09T15:53:40.294804Z",
     "shell.execute_reply": "2024-06-09T15:53:40.293939Z"
    },
    "papermill": {
     "duration": 26915.059924,
     "end_time": "2024-06-09T15:53:40.329145",
     "exception": false,
     "start_time": "2024-06-09T08:25:05.269221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: /kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\n",
      "Device: cuda\n",
      "Training started...\n",
      "****************************************************************************************************\n",
      "Epoch [1/50]:\n",
      "\t Batch [0/800], Train Loss: 0.8919\n",
      "\t Batch [200/800], Train Loss: 0.2356\n",
      "\t Batch [400/800], Train Loss: 0.2329\n",
      "\t Batch [600/800], Train Loss: 0.2094\n",
      "Epoch [1/50], Validation Batch [0/200], Val Loss: 0.2108\n",
      "Epoch [1/50], Validation Batch [100/200], Val Loss: 0.1776\n",
      "Validation, Train Loss: 0.2599, Val Loss: 0.1999\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [2/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1855\n",
      "\t Batch [200/800], Train Loss: 0.1896\n",
      "\t Batch [400/800], Train Loss: 0.2025\n",
      "\t Batch [600/800], Train Loss: 0.1828\n",
      "Epoch [2/50], Validation Batch [0/200], Val Loss: 0.1842\n",
      "Epoch [2/50], Validation Batch [100/200], Val Loss: 0.1602\n",
      "Validation, Train Loss: 0.1863, Val Loss: 0.1875\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [3/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1755\n",
      "\t Batch [200/800], Train Loss: 0.1741\n",
      "\t Batch [400/800], Train Loss: 0.1806\n",
      "\t Batch [600/800], Train Loss: 0.1893\n",
      "Epoch [3/50], Validation Batch [0/200], Val Loss: 0.1794\n",
      "Epoch [3/50], Validation Batch [100/200], Val Loss: 0.1548\n",
      "Validation, Train Loss: 0.1784, Val Loss: 0.1830\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [4/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1700\n",
      "\t Batch [200/800], Train Loss: 0.1668\n",
      "\t Batch [400/800], Train Loss: 0.1812\n",
      "\t Batch [600/800], Train Loss: 0.1722\n",
      "Epoch [4/50], Validation Batch [0/200], Val Loss: 0.1868\n",
      "Epoch [4/50], Validation Batch [100/200], Val Loss: 0.1496\n",
      "Validation, Train Loss: 0.1739, Val Loss: 0.1791\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [5/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1777\n",
      "\t Batch [200/800], Train Loss: 0.1695\n",
      "\t Batch [400/800], Train Loss: 0.1613\n",
      "\t Batch [600/800], Train Loss: 0.1680\n",
      "Epoch [5/50], Validation Batch [0/200], Val Loss: 0.1688\n",
      "Epoch [5/50], Validation Batch [100/200], Val Loss: 0.1491\n",
      "Validation, Train Loss: 0.1713, Val Loss: 0.1760\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [6/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1804\n",
      "\t Batch [200/800], Train Loss: 0.1731\n",
      "\t Batch [400/800], Train Loss: 0.1581\n",
      "\t Batch [600/800], Train Loss: 0.1756\n",
      "Epoch [6/50], Validation Batch [0/200], Val Loss: 0.1752\n",
      "Epoch [6/50], Validation Batch [100/200], Val Loss: 0.1555\n",
      "Validation, Train Loss: 0.1696, Val Loss: 0.1751\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [7/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1576\n",
      "\t Batch [200/800], Train Loss: 0.1558\n",
      "\t Batch [400/800], Train Loss: 0.1610\n",
      "\t Batch [600/800], Train Loss: 0.1623\n",
      "Epoch [7/50], Validation Batch [0/200], Val Loss: 0.1737\n",
      "Epoch [7/50], Validation Batch [100/200], Val Loss: 0.1534\n",
      "Validation, Train Loss: 0.1681, Val Loss: 0.1756\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [8/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1675\n",
      "\t Batch [200/800], Train Loss: 0.1707\n",
      "\t Batch [400/800], Train Loss: 0.1766\n",
      "\t Batch [600/800], Train Loss: 0.1672\n",
      "Epoch [8/50], Validation Batch [0/200], Val Loss: 0.1766\n",
      "Epoch [8/50], Validation Batch [100/200], Val Loss: 0.1468\n",
      "Validation, Train Loss: 0.1666, Val Loss: 0.1721\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [9/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1724\n",
      "\t Batch [200/800], Train Loss: 0.1586\n",
      "\t Batch [400/800], Train Loss: 0.1732\n",
      "\t Batch [600/800], Train Loss: 0.1497\n",
      "Epoch [9/50], Validation Batch [0/200], Val Loss: 0.1774\n",
      "Epoch [9/50], Validation Batch [100/200], Val Loss: 0.1531\n",
      "Validation, Train Loss: 0.1658, Val Loss: 0.1727\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [10/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1434\n",
      "\t Batch [200/800], Train Loss: 0.1980\n",
      "\t Batch [400/800], Train Loss: 0.1612\n",
      "\t Batch [600/800], Train Loss: 0.1466\n",
      "Epoch [10/50], Validation Batch [0/200], Val Loss: 0.1710\n",
      "Epoch [10/50], Validation Batch [100/200], Val Loss: 0.1438\n",
      "Validation, Train Loss: 0.1651, Val Loss: 0.1754\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [11/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1694\n",
      "\t Batch [200/800], Train Loss: 0.1509\n",
      "\t Batch [400/800], Train Loss: 0.1724\n",
      "\t Batch [600/800], Train Loss: 0.1552\n",
      "Epoch [11/50], Validation Batch [0/200], Val Loss: 0.1701\n",
      "Epoch [11/50], Validation Batch [100/200], Val Loss: 0.1481\n",
      "Validation, Train Loss: 0.1626, Val Loss: 0.1709\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [12/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1771\n",
      "\t Batch [200/800], Train Loss: 0.1596\n",
      "\t Batch [400/800], Train Loss: 0.1597\n",
      "\t Batch [600/800], Train Loss: 0.1606\n",
      "Epoch [12/50], Validation Batch [0/200], Val Loss: 0.1657\n",
      "Epoch [12/50], Validation Batch [100/200], Val Loss: 0.1441\n",
      "Validation, Train Loss: 0.1623, Val Loss: 0.1695\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [13/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1393\n",
      "\t Batch [200/800], Train Loss: 0.1558\n",
      "\t Batch [400/800], Train Loss: 0.1588\n",
      "\t Batch [600/800], Train Loss: 0.1588\n",
      "Epoch [13/50], Validation Batch [0/200], Val Loss: 0.1709\n",
      "Epoch [13/50], Validation Batch [100/200], Val Loss: 0.1606\n",
      "Validation, Train Loss: 0.1621, Val Loss: 0.1742\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [14/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1582\n",
      "\t Batch [200/800], Train Loss: 0.1473\n",
      "\t Batch [400/800], Train Loss: 0.1476\n",
      "\t Batch [600/800], Train Loss: 0.1554\n",
      "Epoch [14/50], Validation Batch [0/200], Val Loss: 0.1766\n",
      "Epoch [14/50], Validation Batch [100/200], Val Loss: 0.1453\n",
      "Validation, Train Loss: 0.1618, Val Loss: 0.1706\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [15/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1497\n",
      "\t Batch [200/800], Train Loss: 0.1632\n",
      "\t Batch [400/800], Train Loss: 0.1699\n",
      "\t Batch [600/800], Train Loss: 0.1671\n",
      "Epoch [15/50], Validation Batch [0/200], Val Loss: 0.1687\n",
      "Epoch [15/50], Validation Batch [100/200], Val Loss: 0.1477\n",
      "Validation, Train Loss: 0.1611, Val Loss: 0.1692\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [16/50]:\n",
      "\t Batch [0/800], Train Loss: 0.2067\n",
      "\t Batch [200/800], Train Loss: 0.1480\n",
      "\t Batch [400/800], Train Loss: 0.1664\n",
      "\t Batch [600/800], Train Loss: 0.1448\n",
      "Epoch [16/50], Validation Batch [0/200], Val Loss: 0.1764\n",
      "Epoch [16/50], Validation Batch [100/200], Val Loss: 0.1506\n",
      "Validation, Train Loss: 0.1608, Val Loss: 0.1690\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [17/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1737\n",
      "\t Batch [200/800], Train Loss: 0.1541\n",
      "\t Batch [400/800], Train Loss: 0.1808\n",
      "\t Batch [600/800], Train Loss: 0.1470\n",
      "Epoch [17/50], Validation Batch [0/200], Val Loss: 0.1655\n",
      "Epoch [17/50], Validation Batch [100/200], Val Loss: 0.1493\n",
      "Validation, Train Loss: 0.1604, Val Loss: 0.1687\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [18/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1556\n",
      "\t Batch [200/800], Train Loss: 0.1615\n",
      "\t Batch [400/800], Train Loss: 0.1658\n",
      "\t Batch [600/800], Train Loss: 0.1718\n",
      "Epoch [18/50], Validation Batch [0/200], Val Loss: 0.1705\n",
      "Epoch [18/50], Validation Batch [100/200], Val Loss: 0.1456\n",
      "Validation, Train Loss: 0.1606, Val Loss: 0.1678\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [19/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1518\n",
      "\t Batch [200/800], Train Loss: 0.1673\n",
      "\t Batch [400/800], Train Loss: 0.1599\n",
      "\t Batch [600/800], Train Loss: 0.1668\n",
      "Epoch [19/50], Validation Batch [0/200], Val Loss: 0.1737\n",
      "Epoch [19/50], Validation Batch [100/200], Val Loss: 0.1482\n",
      "Validation, Train Loss: 0.1600, Val Loss: 0.1687\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [20/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1609\n",
      "\t Batch [200/800], Train Loss: 0.1626\n",
      "\t Batch [400/800], Train Loss: 0.1497\n",
      "\t Batch [600/800], Train Loss: 0.1877\n",
      "Epoch [20/50], Validation Batch [0/200], Val Loss: 0.1701\n",
      "Epoch [20/50], Validation Batch [100/200], Val Loss: 0.1419\n",
      "Validation, Train Loss: 0.1600, Val Loss: 0.1686\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [21/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1511\n",
      "\t Batch [200/800], Train Loss: 0.1478\n",
      "\t Batch [400/800], Train Loss: 0.1499\n",
      "\t Batch [600/800], Train Loss: 0.1538\n",
      "Epoch [21/50], Validation Batch [0/200], Val Loss: 0.1612\n",
      "Epoch [21/50], Validation Batch [100/200], Val Loss: 0.1414\n",
      "Validation, Train Loss: 0.1585, Val Loss: 0.1663\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [22/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1439\n",
      "\t Batch [200/800], Train Loss: 0.1449\n",
      "\t Batch [400/800], Train Loss: 0.1567\n",
      "\t Batch [600/800], Train Loss: 0.1517\n",
      "Epoch [22/50], Validation Batch [0/200], Val Loss: 0.1728\n",
      "Epoch [22/50], Validation Batch [100/200], Val Loss: 0.1457\n",
      "Validation, Train Loss: 0.1585, Val Loss: 0.1678\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [23/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1627\n",
      "\t Batch [200/800], Train Loss: 0.1536\n",
      "\t Batch [400/800], Train Loss: 0.1500\n",
      "\t Batch [600/800], Train Loss: 0.1598\n",
      "Epoch [23/50], Validation Batch [0/200], Val Loss: 0.1689\n",
      "Epoch [23/50], Validation Batch [100/200], Val Loss: 0.1403\n",
      "Validation, Train Loss: 0.1587, Val Loss: 0.1666\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [24/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1448\n",
      "\t Batch [200/800], Train Loss: 0.1582\n",
      "\t Batch [400/800], Train Loss: 0.1722\n",
      "\t Batch [600/800], Train Loss: 0.1544\n",
      "Epoch [24/50], Validation Batch [0/200], Val Loss: 0.1726\n",
      "Epoch [24/50], Validation Batch [100/200], Val Loss: 0.1373\n",
      "Validation, Train Loss: 0.1582, Val Loss: 0.1673\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [25/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1445\n",
      "\t Batch [200/800], Train Loss: 0.1742\n",
      "\t Batch [400/800], Train Loss: 0.1453\n",
      "\t Batch [600/800], Train Loss: 0.1757\n",
      "Epoch [25/50], Validation Batch [0/200], Val Loss: 0.1689\n",
      "Epoch [25/50], Validation Batch [100/200], Val Loss: 0.1455\n",
      "Validation, Train Loss: 0.1581, Val Loss: 0.1675\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [26/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1604\n",
      "\t Batch [200/800], Train Loss: 0.1624\n",
      "\t Batch [400/800], Train Loss: 0.1427\n",
      "\t Batch [600/800], Train Loss: 0.1866\n",
      "Epoch [26/50], Validation Batch [0/200], Val Loss: 0.1656\n",
      "Epoch [26/50], Validation Batch [100/200], Val Loss: 0.1339\n",
      "Validation, Train Loss: 0.1580, Val Loss: 0.1671\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [27/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1610\n",
      "\t Batch [200/800], Train Loss: 0.1623\n",
      "\t Batch [400/800], Train Loss: 0.1467\n",
      "\t Batch [600/800], Train Loss: 0.1723\n",
      "Epoch [27/50], Validation Batch [0/200], Val Loss: 0.1663\n",
      "Epoch [27/50], Validation Batch [100/200], Val Loss: 0.1499\n",
      "Validation, Train Loss: 0.1580, Val Loss: 0.1670\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [28/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1609\n",
      "\t Batch [200/800], Train Loss: 0.1506\n",
      "\t Batch [400/800], Train Loss: 0.1541\n",
      "\t Batch [600/800], Train Loss: 0.1730\n",
      "Epoch [28/50], Validation Batch [0/200], Val Loss: 0.1658\n",
      "Epoch [28/50], Validation Batch [100/200], Val Loss: 0.1404\n",
      "Validation, Train Loss: 0.1581, Val Loss: 0.1677\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [29/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1437\n",
      "\t Batch [200/800], Train Loss: 0.1682\n",
      "\t Batch [400/800], Train Loss: 0.1427\n",
      "\t Batch [600/800], Train Loss: 0.1601\n",
      "Epoch [29/50], Validation Batch [0/200], Val Loss: 0.1633\n",
      "Epoch [29/50], Validation Batch [100/200], Val Loss: 0.1358\n",
      "Validation, Train Loss: 0.1578, Val Loss: 0.1670\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [30/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1635\n",
      "\t Batch [200/800], Train Loss: 0.1463\n",
      "\t Batch [400/800], Train Loss: 0.1576\n",
      "\t Batch [600/800], Train Loss: 0.1643\n",
      "Epoch [30/50], Validation Batch [0/200], Val Loss: 0.1647\n",
      "Epoch [30/50], Validation Batch [100/200], Val Loss: 0.1382\n",
      "Validation, Train Loss: 0.1577, Val Loss: 0.1658\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [31/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1651\n",
      "\t Batch [200/800], Train Loss: 0.1678\n",
      "\t Batch [400/800], Train Loss: 0.1690\n",
      "\t Batch [600/800], Train Loss: 0.1548\n",
      "Epoch [31/50], Validation Batch [0/200], Val Loss: 0.1644\n",
      "Epoch [31/50], Validation Batch [100/200], Val Loss: 0.1421\n",
      "Validation, Train Loss: 0.1576, Val Loss: 0.1660\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [32/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1716\n",
      "\t Batch [200/800], Train Loss: 0.1443\n",
      "\t Batch [400/800], Train Loss: 0.1666\n",
      "\t Batch [600/800], Train Loss: 0.1484\n",
      "Epoch [32/50], Validation Batch [0/200], Val Loss: 0.1666\n",
      "Epoch [32/50], Validation Batch [100/200], Val Loss: 0.1494\n",
      "Validation, Train Loss: 0.1577, Val Loss: 0.1662\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [33/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1556\n",
      "\t Batch [200/800], Train Loss: 0.1592\n",
      "\t Batch [400/800], Train Loss: 0.1460\n",
      "\t Batch [600/800], Train Loss: 0.1580\n",
      "Epoch [33/50], Validation Batch [0/200], Val Loss: 0.1609\n",
      "Epoch [33/50], Validation Batch [100/200], Val Loss: 0.1351\n",
      "Validation, Train Loss: 0.1570, Val Loss: 0.1660\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [34/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1493\n",
      "\t Batch [200/800], Train Loss: 0.1677\n",
      "\t Batch [400/800], Train Loss: 0.1505\n",
      "\t Batch [600/800], Train Loss: 0.1611\n",
      "Epoch [34/50], Validation Batch [0/200], Val Loss: 0.1703\n",
      "Epoch [34/50], Validation Batch [100/200], Val Loss: 0.1307\n",
      "Validation, Train Loss: 0.1568, Val Loss: 0.1654\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [35/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1645\n",
      "\t Batch [200/800], Train Loss: 0.1713\n",
      "\t Batch [400/800], Train Loss: 0.1642\n",
      "\t Batch [600/800], Train Loss: 0.1549\n",
      "Epoch [35/50], Validation Batch [0/200], Val Loss: 0.1697\n",
      "Epoch [35/50], Validation Batch [100/200], Val Loss: 0.1428\n",
      "Validation, Train Loss: 0.1573, Val Loss: 0.1652\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [36/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1676\n",
      "\t Batch [200/800], Train Loss: 0.1597\n",
      "\t Batch [400/800], Train Loss: 0.1384\n",
      "\t Batch [600/800], Train Loss: 0.1536\n",
      "Epoch [36/50], Validation Batch [0/200], Val Loss: 0.1756\n",
      "Epoch [36/50], Validation Batch [100/200], Val Loss: 0.1395\n",
      "Validation, Train Loss: 0.1566, Val Loss: 0.1646\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [37/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1469\n",
      "\t Batch [200/800], Train Loss: 0.1534\n",
      "\t Batch [400/800], Train Loss: 0.1446\n",
      "\t Batch [600/800], Train Loss: 0.1628\n",
      "Epoch [37/50], Validation Batch [0/200], Val Loss: 0.1684\n",
      "Epoch [37/50], Validation Batch [100/200], Val Loss: 0.1370\n",
      "Validation, Train Loss: 0.1565, Val Loss: 0.1654\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [38/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1710\n",
      "\t Batch [200/800], Train Loss: 0.1594\n",
      "\t Batch [400/800], Train Loss: 0.1738\n",
      "\t Batch [600/800], Train Loss: 0.1562\n",
      "Epoch [38/50], Validation Batch [0/200], Val Loss: 0.1646\n",
      "Epoch [38/50], Validation Batch [100/200], Val Loss: 0.1397\n",
      "Validation, Train Loss: 0.1570, Val Loss: 0.1658\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [39/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1682\n",
      "\t Batch [200/800], Train Loss: 0.1850\n",
      "\t Batch [400/800], Train Loss: 0.1659\n",
      "\t Batch [600/800], Train Loss: 0.1562\n",
      "Epoch [39/50], Validation Batch [0/200], Val Loss: 0.1589\n",
      "Epoch [39/50], Validation Batch [100/200], Val Loss: 0.1405\n",
      "Validation, Train Loss: 0.1566, Val Loss: 0.1663\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [40/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1653\n",
      "\t Batch [200/800], Train Loss: 0.1622\n",
      "\t Batch [400/800], Train Loss: 0.1644\n",
      "\t Batch [600/800], Train Loss: 0.1610\n",
      "Epoch [40/50], Validation Batch [0/200], Val Loss: 0.1694\n",
      "Epoch [40/50], Validation Batch [100/200], Val Loss: 0.1411\n",
      "Validation, Train Loss: 0.1566, Val Loss: 0.1662\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [41/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1647\n",
      "\t Batch [200/800], Train Loss: 0.1521\n",
      "\t Batch [400/800], Train Loss: 0.1490\n",
      "\t Batch [600/800], Train Loss: 0.1605\n",
      "Epoch [41/50], Validation Batch [0/200], Val Loss: 0.1702\n",
      "Epoch [41/50], Validation Batch [100/200], Val Loss: 0.1457\n",
      "Validation, Train Loss: 0.1562, Val Loss: 0.1652\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [42/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1539\n",
      "\t Batch [200/800], Train Loss: 0.1587\n",
      "\t Batch [400/800], Train Loss: 0.1598\n",
      "\t Batch [600/800], Train Loss: 0.1272\n",
      "Epoch [42/50], Validation Batch [0/200], Val Loss: 0.1661\n",
      "Epoch [42/50], Validation Batch [100/200], Val Loss: 0.1382\n",
      "Validation, Train Loss: 0.1560, Val Loss: 0.1655\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [43/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1450\n",
      "\t Batch [200/800], Train Loss: 0.1465\n",
      "\t Batch [400/800], Train Loss: 0.1799\n",
      "\t Batch [600/800], Train Loss: 0.1624\n",
      "Epoch [43/50], Validation Batch [0/200], Val Loss: 0.1624\n",
      "Epoch [43/50], Validation Batch [100/200], Val Loss: 0.1457\n",
      "Validation, Train Loss: 0.1564, Val Loss: 0.1649\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [44/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1524\n",
      "\t Batch [200/800], Train Loss: 0.1570\n",
      "\t Batch [400/800], Train Loss: 0.1585\n",
      "\t Batch [600/800], Train Loss: 0.1711\n",
      "Epoch [44/50], Validation Batch [0/200], Val Loss: 0.1612\n",
      "Epoch [44/50], Validation Batch [100/200], Val Loss: 0.1375\n",
      "Validation, Train Loss: 0.1558, Val Loss: 0.1648\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [45/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1591\n",
      "\t Batch [200/800], Train Loss: 0.1685\n",
      "\t Batch [400/800], Train Loss: 0.1532\n",
      "\t Batch [600/800], Train Loss: 0.1486\n",
      "Epoch [45/50], Validation Batch [0/200], Val Loss: 0.1671\n",
      "Epoch [45/50], Validation Batch [100/200], Val Loss: 0.1411\n",
      "Validation, Train Loss: 0.1563, Val Loss: 0.1652\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [46/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1598\n",
      "\t Batch [200/800], Train Loss: 0.1575\n",
      "\t Batch [400/800], Train Loss: 0.1680\n",
      "\t Batch [600/800], Train Loss: 0.1716\n",
      "Epoch [46/50], Validation Batch [0/200], Val Loss: 0.1708\n",
      "Epoch [46/50], Validation Batch [100/200], Val Loss: 0.1332\n",
      "Validation, Train Loss: 0.1563, Val Loss: 0.1650\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [47/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1422\n",
      "\t Batch [200/800], Train Loss: 0.1427\n",
      "\t Batch [400/800], Train Loss: 0.1480\n",
      "\t Batch [600/800], Train Loss: 0.1629\n",
      "Epoch [47/50], Validation Batch [0/200], Val Loss: 0.1650\n",
      "Epoch [47/50], Validation Batch [100/200], Val Loss: 0.1409\n",
      "Validation, Train Loss: 0.1563, Val Loss: 0.1652\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [48/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1782\n",
      "\t Batch [200/800], Train Loss: 0.1488\n",
      "\t Batch [400/800], Train Loss: 0.1591\n",
      "\t Batch [600/800], Train Loss: 0.1521\n",
      "Epoch [48/50], Validation Batch [0/200], Val Loss: 0.1617\n",
      "Epoch [48/50], Validation Batch [100/200], Val Loss: 0.1416\n",
      "Validation, Train Loss: 0.1562, Val Loss: 0.1654\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [49/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1560\n",
      "\t Batch [200/800], Train Loss: 0.1608\n",
      "\t Batch [400/800], Train Loss: 0.1474\n",
      "\t Batch [600/800], Train Loss: 0.1639\n",
      "Epoch [49/50], Validation Batch [0/200], Val Loss: 0.1680\n",
      "Epoch [49/50], Validation Batch [100/200], Val Loss: 0.1482\n",
      "Validation, Train Loss: 0.1561, Val Loss: 0.1653\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch [50/50]:\n",
      "\t Batch [0/800], Train Loss: 0.1492\n",
      "\t Batch [200/800], Train Loss: 0.1648\n",
      "\t Batch [400/800], Train Loss: 0.1594\n",
      "\t Batch [600/800], Train Loss: 0.1514\n",
      "Epoch [50/50], Validation Batch [0/200], Val Loss: 0.1683\n",
      "Epoch [50/50], Validation Batch [100/200], Val Loss: 0.1358\n",
      "Validation, Train Loss: 0.1562, Val Loss: 0.1647\n",
      "****************************************************************************************************\n",
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SimMIM(\n",
       "   (encoder): VisionTransformerForSimMIM(\n",
       "     (patch_embed): PatchEmbed(\n",
       "       (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "     )\n",
       "     (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "     (rel_pos_bias): RelativePositionBias()\n",
       "     (blocks): ModuleList(\n",
       "       (0): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): Identity()\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.009)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.018)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.027)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.036)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.045)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (6): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.055)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (7): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.064)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (8): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.073)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (9): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.082)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (10): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.091)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (11): Block(\n",
       "         (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (attn): Attention(\n",
       "           (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (drop_path): DropPath(drop_prob=0.100)\n",
       "         (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (act): GELU(approximate='none')\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "   )\n",
       "   (decoder): Sequential(\n",
       "     (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "     (1): PixelShuffle(upscale_factor=16)\n",
       "   )\n",
       " ),\n",
       " [0.25991742545738816,\n",
       "  0.18625109158456327,\n",
       "  0.17837740380316972,\n",
       "  0.17389688031747938,\n",
       "  0.17126740073785185,\n",
       "  0.16963422559201718,\n",
       "  0.16811621204018593,\n",
       "  0.1666189811937511,\n",
       "  0.16581387355923652,\n",
       "  0.1650812991335988,\n",
       "  0.16263161245733498,\n",
       "  0.162304166238755,\n",
       "  0.16206722594797612,\n",
       "  0.1618057817593217,\n",
       "  0.1611139632202685,\n",
       "  0.16078030213713645,\n",
       "  0.1604458844847977,\n",
       "  0.16060879422351718,\n",
       "  0.1599752612411976,\n",
       "  0.15999241748824716,\n",
       "  0.158508154861629,\n",
       "  0.15848836895078422,\n",
       "  0.15865723747760058,\n",
       "  0.15821124209091067,\n",
       "  0.15810441264882683,\n",
       "  0.15801768870092928,\n",
       "  0.15801951911300421,\n",
       "  0.15808417744934558,\n",
       "  0.15784671350382268,\n",
       "  0.15773085970431566,\n",
       "  0.1575891236588359,\n",
       "  0.15772168533876538,\n",
       "  0.1570006825774908,\n",
       "  0.15675569888204335,\n",
       "  0.1572881433367729,\n",
       "  0.15659403836354613,\n",
       "  0.15653882456943394,\n",
       "  0.15702341318130494,\n",
       "  0.15656777773052455,\n",
       "  0.15657600175589323,\n",
       "  0.15623155796900393,\n",
       "  0.15599959304556252,\n",
       "  0.1564108996093273,\n",
       "  0.1558458803035319,\n",
       "  0.1563424397446215,\n",
       "  0.15625927528366446,\n",
       "  0.15625738751143217,\n",
       "  0.15623962216079235,\n",
       "  0.15612123100087047,\n",
       "  0.15618479343131184],\n",
       " [0.19994013532996177,\n",
       "  0.1874657728523016,\n",
       "  0.18304135851562023,\n",
       "  0.17905795633792876,\n",
       "  0.1760065445303917,\n",
       "  0.17508389078080655,\n",
       "  0.17563114419579506,\n",
       "  0.17214281521737576,\n",
       "  0.17266137905418874,\n",
       "  0.17536975212395192,\n",
       "  0.17092528216540814,\n",
       "  0.16946538627147675,\n",
       "  0.17424928069114684,\n",
       "  0.17060428373515607,\n",
       "  0.16918454736471175,\n",
       "  0.16895010374486447,\n",
       "  0.16869701996445655,\n",
       "  0.16780054658651353,\n",
       "  0.1686824479699135,\n",
       "  0.16864157475531102,\n",
       "  0.16628468930721282,\n",
       "  0.16779057547450066,\n",
       "  0.1665879724174738,\n",
       "  0.16730114057660103,\n",
       "  0.16748040273785592,\n",
       "  0.16708034895360468,\n",
       "  0.16703275486826896,\n",
       "  0.167744801864028,\n",
       "  0.1670035684108734,\n",
       "  0.16578806214034558,\n",
       "  0.16603247173130511,\n",
       "  0.16621562197804451,\n",
       "  0.16604398988187313,\n",
       "  0.16544022731482982,\n",
       "  0.16517073407769203,\n",
       "  0.16458868801593782,\n",
       "  0.16538538977503778,\n",
       "  0.1658258794993162,\n",
       "  0.16628424540162087,\n",
       "  0.16624409064650536,\n",
       "  0.1651891865581274,\n",
       "  0.16548083402216435,\n",
       "  0.16491393193602563,\n",
       "  0.16482542313635348,\n",
       "  0.16519751764833926,\n",
       "  0.16501581855118275,\n",
       "  0.16519367583096028,\n",
       "  0.165363162830472,\n",
       "  0.16532338015735148,\n",
       "  0.16465674161911012])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "train_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n",
    "                                metadata = config[\"annotation_data_path\"],\n",
    "                                phase = \"training\",\n",
    "                                seed=0)\n",
    "valid_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n",
    "                                metadata = config[\"annotation_data_path\"],\n",
    "                                phase = \"valid\",\n",
    "                                seed=0)\n",
    "with open(config[\"model_config\"], 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "model_config = AttrDict(data)\n",
    "simmim_model = build_simmim(model_config)\n",
    "\n",
    "if config[\"checkpoint\"]:\n",
    "    checkpoint = torch.load(config[\"checkpoint\"])\n",
    "    print(\"Checkpoint: {}\".format(config[\"checkpoint\"]))\n",
    "    simmim_model.encoder.load_state_dict(checkpoint, strict = False)\n",
    "\n",
    "train_model(model=simmim_model, train_dataset=train_dataset,\n",
    "            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n",
    "            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "            checkpoint_folder=config[\"checkpoint_folder\"]\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5121102,
     "sourceId": 8566029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5170605,
     "sourceId": 8634739,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 52230,
     "sourceId": 62544,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26926.922631,
   "end_time": "2024-06-09T15:53:42.721682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-09T08:24:55.799051",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
