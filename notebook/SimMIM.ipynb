{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8566029,"sourceType":"datasetVersion","datasetId":5121102},{"sourceId":8634739,"sourceType":"datasetVersion","datasetId":5170605},{"sourceId":62544,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52230}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport cv2\nimport os\nfrom torchvision import transforms\nimport pandas as pd\nfrom torch.utils.data._utils.collate import default_collate\n\n\ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nmean = [0.2652, 0.2652, 0.2652]\nstd = [0.1994, 0.1994, 0.1994]\ndata_transforms = {\n    'training': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3),\n        transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3), ]), p=0.3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'valid': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'test': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n\n\nclass MaskGenerator:\n    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n        self.input_size = input_size\n        self.mask_patch_size = mask_patch_size\n        self.model_patch_size = model_patch_size\n        self.mask_ratio = mask_ratio\n\n        assert self.input_size % self.mask_patch_size == 0\n        assert self.mask_patch_size % self.model_patch_size == 0\n\n        self.rand_size = self.input_size // self.mask_patch_size\n        self.scale = self.mask_patch_size // self.model_patch_size\n\n        self.token_count = self.rand_size ** 2\n        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n\n    def __call__(self):\n        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n        mask = np.zeros(self.token_count, dtype=int)\n        mask[mask_idx] = 1\n\n        mask = mask.reshape((self.rand_size, self.rand_size))\n        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n\n        return mask\n\n\nclass MammoDataset(Dataset):\n    def __init__(self,\n                 data_path,\n                 metadata,\n                 phase,\n                 trasnform=None,\n                 certain=True,\n                 image_size=224,\n                 mask_patch_size=32,\n                 model_patch_size=16,\n                 mask_ratio=0.6,\n                 seed=None):\n        self.phase = phase\n        self.certain = certain\n        self.data_path = data_path\n\n        if (seed):\n            seed_everything(seed)\n\n        self.transform = data_transforms[self.phase] if (trasnform is None) else trasnform\n        data = pd.read_csv(metadata)\n        self.data = data.loc[data[\"split\"] == phase].reset_index()\n        self.mask_generator = MaskGenerator(\n            input_size=image_size,\n            mask_patch_size=mask_patch_size,\n            model_patch_size=model_patch_size,\n            mask_ratio=mask_ratio,\n        )\n\n    def get_path(self, data, index):\n        image_name = data['image_id'].iloc[index]\n        study_id = data['study_id'].iloc[index]\n        image_path = os.path.join(self.data_path, study_id + '/' + image_name + '.png')\n        return image_path\n\n    def get_score(self, data, index):\n        birads = data['breast_birads'].iloc[index]\n        score = eval(birads[-1])\n        return score\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        image_path = self.get_path(self.data, index)\n        image = cv2.imread(image_path)\n        image = self.transform(image)\n        mask = self.mask_generator()\n        mask = transforms.ToTensor()(mask)\n        score = self.get_score(self.data, index)\n        return image, mask, score\n\n\ndef collate_fn(batch):\n    if not isinstance(batch[0][0], tuple):\n        return default_collate(batch)\n    else:\n        batch_num = len(batch)\n        ret = []\n        for item_idx in range(len(batch[0][0])):\n            if batch[0][0][item_idx] is None:\n                ret.append(None)\n            else:\n                ret.append(default_collate([batch[i][0][item_idx] for i in range(batch_num)]))\n        ret.append(default_collate([batch[i][1] for i in range(batch_num)]))\n        return ret\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:11.188141Z","iopub.execute_input":"2024-06-09T08:16:11.188458Z","iopub.status.idle":"2024-06-09T08:16:17.693681Z","shell.execute_reply.started":"2024-06-09T08:16:11.188431Z","shell.execute_reply":"2024-06-09T08:16:17.692908Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model","metadata":{}},{"cell_type":"markdown","source":"## 2.1 - ViT Encoder","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on BEIT code bases (https://github.com/microsoft/unilm/tree/master/beit)\n# Written by Yutong Lin, Zhenda Xie\n# --------------------------------------------------------\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # comment out this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            # cls to token & token to cls & cls to cls\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n                 use_mean_pooling=True, init_scale=0.001):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n\n        if self.pos_embed is not None:\n            self._trunc_normal_(self.pos_embed, std=.02)\n        self._trunc_normal_(self.cls_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def _trunc_normal_(self, tensor, mean=0., std=1.):\n        trunc_normal_(tensor, mean=mean, std=std)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            self._trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            self._trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        x = self.norm(x)\n        if self.fc_norm is not None:\n            t = x[:, 1:, :]\n            return self.fc_norm(t.mean(1))\n        else:\n            return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef build_vit(config):\n    model = VisionTransformer(\n        img_size=config.DATA.IMG_SIZE,\n        patch_size=config.MODEL.VIT.PATCH_SIZE,\n        in_chans=config.MODEL.VIT.IN_CHANS,\n        embed_dim=config.MODEL.VIT.EMBED_DIM,\n        depth=config.MODEL.VIT.DEPTH,\n        num_heads=config.MODEL.VIT.NUM_HEADS,\n        mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n        qkv_bias=config.MODEL.VIT.QKV_BIAS,\n        drop_rate=config.MODEL.DROP_RATE,\n        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=config.MODEL.VIT.INIT_VALUES,\n        use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n        use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n        use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n        use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:17.695716Z","iopub.execute_input":"2024-06-09T08:16:17.696107Z","iopub.status.idle":"2024-06-09T08:16:18.946481Z","shell.execute_reply.started":"2024-06-09T08:16:17.696081Z","shell.execute_reply":"2024-06-09T08:16:18.945471Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 - SimMIM model","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Zhenda Xie\n# --------------------------------------------------------\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import trunc_normal_\n\n\n\n# class SwinTransformerForSimMIM(SwinTransformer):\n#     def __init__(self, **kwargs):\n#         super().__init__(**kwargs)\n\n#         assert self.num_classes == 0\n\n#         self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n#         trunc_normal_(self.mask_token, mean=0., std=.02)\n\n#     def forward(self, x, mask):\n#         x = self.patch_embed(x)\n\n#         assert mask is not None\n#         B, L, _ = x.shape\n\n#         mask_tokens = self.mask_token.expand(B, L, -1)\n#         w = mask.flatten(1).unsqueeze(-1).type_as(mask_tokens)\n#         x = x * (1. - w) + mask_tokens * w\n\n#         if self.ape:\n#             x = x + self.absolute_pos_embed\n#         x = self.pos_drop(x)\n\n#         for layer in self.layers:\n#             x = layer(x)\n#         x = self.norm(x)\n\n#         x = x.transpose(1, 2)\n#         B, C, L = x.shape\n#         H = W = int(L ** 0.5)\n#         x = x.reshape(B, C, H, W)\n#         return x\n\n#     @torch.jit.ignore\n#     def no_weight_decay(self):\n#         return super().no_weight_decay() | {'mask_token'}\n\n\nclass VisionTransformerForSimMIM(VisionTransformer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        self._trunc_normal_(self.mask_token, std=.02)\n\n    def _trunc_normal_(self, tensor, mean=0., std=1.):\n        trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n\n    def forward(self, x, mask):\n        x = self.patch_embed(x)\n\n        assert mask is not None\n        B, L, _ = x.shape\n\n        mask_token = self.mask_token.expand(B, L, -1)\n        w = mask.flatten(1).unsqueeze(-1).type_as(mask_token)\n        x = x * (1 - w) + mask_token * w\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias=rel_pos_bias)\n        x = self.norm(x)\n\n        x = x[:, 1:]\n        B, L, C = x.shape\n        H = W = int(L ** 0.5)\n        x = x.permute(0, 2, 1).reshape(B, C, H, W)\n        return x\n\n\nclass SimMIM(nn.Module):\n    def __init__(self, encoder, encoder_stride):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder_stride = encoder_stride\n\n        self.decoder = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.encoder.num_features,\n                out_channels=self.encoder_stride ** 2 * 3, kernel_size=1),\n            nn.PixelShuffle(self.encoder_stride),\n        )\n\n        self.in_chans = self.encoder.in_chans\n        self.patch_size = self.encoder.patch_size\n\n    def forward(self, x, mask):\n        z = self.encoder(x, mask)\n        x_rec = self.decoder(z)\n        ## Fix size of mask\n        mask = mask.repeat_interleave(self.patch_size, 2).repeat_interleave(self.patch_size, 3)\n        loss_recon = F.l1_loss(x, x_rec, reduction='none')\n        loss = (loss_recon * mask).sum() / (mask.sum() + 1e-5) / self.in_chans\n        return loss\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        if hasattr(self.encoder, 'no_weight_decay'):\n            return {'encoder.' + i for i in self.encoder.no_weight_decay()}\n        return {}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        if hasattr(self.encoder, 'no_weight_decay_keywords'):\n            return {'encoder.' + i for i in self.encoder.no_weight_decay_keywords()}\n        return {}\n\n\ndef build_simmim(config):\n    model_type = config.MODEL.TYPE\n    if model_type == 'swin':\n        encoder = SwinTransformerForSimMIM(\n            img_size=config.DATA.IMG_SIZE,\n            patch_size=config.MODEL.SWIN.PATCH_SIZE,\n            in_chans=config.MODEL.SWIN.IN_CHANS,\n            embed_dim=config.MODEL.SWIN.EMBED_DIM,\n            depths=config.MODEL.SWIN.DEPTHS,\n            num_heads=config.MODEL.SWIN.NUM_HEADS,\n            window_size=config.MODEL.SWIN.WINDOW_SIZE,\n            mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n            qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n            qk_scale=config.MODEL.SWIN.QK_SCALE,\n            drop_rate=config.MODEL.DROP_RATE,\n            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n            ape=config.MODEL.SWIN.APE,\n            patch_norm=config.MODEL.SWIN.PATCH_NORM,\n            use_checkpoint=config.TRAIN.USE_CHECKPOINT)\n        encoder_stride = 32\n    elif model_type == 'vit':\n        encoder = VisionTransformerForSimMIM(\n            img_size=config.DATA.IMG_SIZE,\n            patch_size=config.MODEL.VIT.PATCH_SIZE,\n            in_chans=config.MODEL.VIT.IN_CHANS,\n            embed_dim=config.MODEL.VIT.EMBED_DIM,\n            depth=config.MODEL.VIT.DEPTH,\n            num_heads=config.MODEL.VIT.NUM_HEADS,\n            mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n            qkv_bias=config.MODEL.VIT.QKV_BIAS,\n            drop_rate=config.MODEL.DROP_RATE,\n            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            init_values=config.MODEL.VIT.INIT_VALUES,\n            use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n            use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n            use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n            use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n        encoder_stride = 16\n    else:\n        raise NotImplementedError(f\"Unknown pre-train model: {model_type}\")\n\n    model = SimMIM(encoder=encoder, encoder_stride=encoder_stride)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:18.947913Z","iopub.execute_input":"2024-06-09T08:16:18.948216Z","iopub.status.idle":"2024-06-09T08:16:18.975180Z","shell.execute_reply.started":"2024-06-09T08:16:18.948169Z","shell.execute_reply":"2024-06-09T08:16:18.974222Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 3. Loss function","metadata":{}},{"cell_type":"markdown","source":"# 4. Train pipeline","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport os\n\n\ndef train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32,\n                learning_rate=0.001):\n    \"\"\"\n    Train the model using the provided datasets.\n\n    Args:\n    - model: The model to be trained\n    - train_dataset: Dataset for training\n    - val_dataset: Dataset for validation\n    - checkpoint_folder: Folder to store checkpoints\n    - num_epochs: Number of epochs for training\n    - batch_size: Batch size for training\n    - learning_rate: Learning rate for optimization\n\n    Returns:\n    - model: Trained model\n    - train_losses: List of training losses\n    - val_losses: List of validation losses\n    \"\"\"\n    # Create the checkpoint folder if it doesn't exist\n    if not os.path.exists(checkpoint_folder):\n        os.makedirs(checkpoint_folder)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    # Define data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Define loss function and optimizer\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n    # Lists to store training and validation losses\n    train_losses = []\n    val_losses = []\n\n    # Variables to keep track of the best model and its performance\n    best_val_loss = float('inf')\n    best_model_state = None\n\n    model = model.to(device)\n    print(\"Training started...\")\n    for epoch in range(num_epochs):\n        torch.cuda.empty_cache()\n        print(\"*\" * 100)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}]:\")\n        model.train()\n        running_train_loss = 0.0\n        for i, (img, mask, _) in enumerate(train_loader):\n            optimizer.zero_grad()\n            # Forward pass\n            img = img.to(device)\n            mask = mask.to(device)\n            loss = model(img, mask)\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            running_train_loss += loss.item()\n\n            if i % 200 == 0:\n                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n\n        # Compute average training loss for the epoch\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # Validation loop\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for i, (img, mask, _) in enumerate(val_loader):\n                img = img.to(device)\n                mask = mask.to(device)\n                loss = model(img, mask)\n                running_val_loss += loss.item()\n\n                if i % 100 == 0:\n                    print(\n                        f\"Epoch [{epoch + 1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n\n        # Compute average validation loss for the epoch\n        epoch_val_loss = running_val_loss / len(val_loader)\n        val_losses.append(epoch_val_loss)\n\n        # Save the model checkpoint for every epoch (last model)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': epoch_val_loss\n        }, os.path.join(checkpoint_folder, f'last.pt'))\n\n        # Save the best model checkpoint based on validation loss\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            best_model_state = model.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': best_model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss\n            }, os.path.join(checkpoint_folder, f'best.pt'))\n\n        # Print progress\n        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n        print(\"*\" * 100)\n        scheduler.step()\n    print(\"Training completed.\")\n\n    return model, train_losses, val_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:18.977767Z","iopub.execute_input":"2024-06-09T08:16:18.978370Z","iopub.status.idle":"2024-06-09T08:16:18.995923Z","shell.execute_reply.started":"2024-06-09T08:16:18.978339Z","shell.execute_reply":"2024-06-09T08:16:18.994902Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 5. Experiments","metadata":{}},{"cell_type":"code","source":"import datetime\nnow = datetime.datetime.now()\n\nconfig = {\n    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n    \"model_encoder\": \"vit\",\n    \"data_length\": 100000,\n    \"embedding_dim\": 256, \n    \"learning_rate\":0.1,\n    \"num_epoch\": 50,\n    \"batch_size\": 16,\n    \"model_config\": \"/kaggle/input/vit-config/simmim_pretrain__vit_base__img224__800ep.yaml\",\n    \"checkpoint\": \"/kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\",\n    \"checkpoint_folder\": f\"/kaggle/working/weights_setting2/resnet50BasedModel_{now}\"\n}\n\nclass AttrDict(dict):\n    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for key, value in self.items():\n            if isinstance(value, dict):\n                value = AttrDict(value)\n            self[key] = value\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError(f\"'AttrDict' object has no attribute '{item}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:18.996836Z","iopub.execute_input":"2024-06-09T08:16:18.997093Z","iopub.status.idle":"2024-06-09T08:16:19.009547Z","shell.execute_reply.started":"2024-06-09T08:16:18.997071Z","shell.execute_reply":"2024-06-09T08:16:19.008770Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import yaml\n\ntrain_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"training\",\n                                seed=0)\nvalid_dataset = MammoDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"valid\",\n                                seed=0)\nwith open(config[\"model_config\"], 'r') as file:\n    data = yaml.safe_load(file)\nmodel_config = AttrDict(data)\nsimmim_model = build_simmim(model_config)\n\nif config[\"checkpoint\"]:\n    checkpoint = torch.load(config[\"checkpoint\"])\n    print(\"Checkpoint: {}\".format(config[\"checkpoint\"]))\n    simmim_model.encoder.load_state_dict(checkpoint, strict = False)\n\ntrain_model(model=simmim_model, train_dataset=train_dataset,\n            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n            checkpoint_folder=config[\"checkpoint_folder\"]\n            )","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:16:19.010422Z","iopub.execute_input":"2024-06-09T08:16:19.010668Z","iopub.status.idle":"2024-06-09T08:24:22.470398Z","shell.execute_reply.started":"2024-06-09T08:16:19.010646Z","shell.execute_reply":"2024-06-09T08:24:22.469229Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint: /kaggle/input/pre-trained-encoder-model/pytorch/vit_224_800e/1/vit_base_image224_800ep.pt\nDevice: cuda\nTraining started...\n****************************************************************************************************\nEpoch [1/50]:\n\t Batch [0/800], Train Loss: 0.9394\n\t Batch [200/800], Train Loss: 0.3206\n\t Batch [400/800], Train Loss: 0.2043\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     19\u001b[0m     simmim_model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimmim_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_folder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, checkpoint_folder, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     63\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 65\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Batch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}